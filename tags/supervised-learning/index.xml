<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>supervised-learning on Simple Made Daily</title>
    <link>https://caioferreira.dev/tags/supervised-learning/</link>
    <description>Recent content in supervised-learning on Simple Made Daily</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <lastBuildDate>Thu, 22 Jun 2023 00:00:00 +0000</lastBuildDate><atom:link href="https://caioferreira.dev/tags/supervised-learning/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Notes on Dos and Don&#39;ts of Machine Learning in Computer Security</title>
      <link>https://caioferreira.dev/posts/notes-on-do-and-donts-of-ml-in-security/notes-on-do-and-donts-of-ml-in-security/</link>
      <pubDate>Thu, 22 Jun 2023 00:00:00 +0000</pubDate>
      
      <guid>https://caioferreira.dev/posts/notes-on-do-and-donts-of-ml-in-security/notes-on-do-and-donts-of-ml-in-security/</guid>
      <description>Following the subject from my last post, Reflections about Supervised Learning on Security, I put down some more thoughts about the implementation of learning-based systems in the Security domain.
This is my extension to the problems and recommendations presented on the paper Dos and Don&amp;rsquo;ts of Machine Learning in Computer Security (Quiring, et al, 2022). I encourage you to also read the paper, as it&amp;rsquo;s excellent and provide a lot of insights about how to better build machine learning models.</description>
      <content:encoded><![CDATA[<p>Following the subject from my last post, <a href="https://caioferreira.dev/posts/reflections-supervised-ml/reflections-supervised-learning-in-security/">Reflections about Supervised Learning on Security</a>, I put down some more thoughts about the implementation of learning-based systems in the Security domain.</p>
<p>This is my extension to the problems and recommendations presented on the paper <a href="https://mlsec.org/docs/2022-sec.pdf">Dos and Don&rsquo;ts of Machine Learning in Computer Security</a> (Quiring, et al, 2022). I encourage you to also read the paper, as it&rsquo;s excellent and provide a lot of insights about how to better build machine learning models.</p>
<h2 id="machine-learning-workflow">Machine learning workflow</h2>
<h3 id="data-collection">Data collection</h3>
<blockquote>
<p>Pitfalls: Sampling Bias, Label Inaccuracy</p>
</blockquote>
<p>As with any other domain, Security is also heavily affected by bad data quality used for training. However, it&rsquo;s often worse in Security because data acquisition of adversary activity is usually hard and the method&rsquo;s used to acquire it will bring a bias.</p>
<p>Let&rsquo;s take for example a honeypot implemented with a vulnerable Apache Server. Even though there are lots of bad actors in the wild, if you have a medium size environment, the volume of data they will produce attacking your honeypot will not come near to the volume of packets in your production network. Also, the TTPs you are going to see will come from threat actors that are used to leverage Apache Server in their kill chain, possibly leaving other adversaries, that may be focusing on other types of vectors, producing different kill chains, out of the dataset.</p>
<p>On top of that, we are usually dealing with lots of examples. So, if we have label inaccuracies, dealing with them is a lot harder. We can&rsquo;t just apply common techniques like using the mode to fill in, because in the Security domain, the adversary is actively trying to mimic the distribution of the benign cases. Therefore, we have very little space for noise, as it would blur even more the distinction between the outcomes.</p>
<p>Besides the recommendations presented in the article, I would add two more:</p>
<ul>
<li>User open and disseminate datasets whenever possible. Dataset sharing is still uncommon on the Security community. We already have some network and host datasets, but there are many more assets nowadays (logs for cloud, kubernetes, CDN, CI/CD, etc), and the technologies on networks and hosts are constantly evolving, posing the need for these datasets to be always updated.</li>
<li>Use a model design that depends less on adversary data, such as we discussed in the previous article. This will reduce the dependency on the adversary behavior and increase the sources of data available to use.</li>
</ul>
<h3 id="model-design-and-implementation">Model design and implementation</h3>
<blockquote>
<p>Pitfalls: Data Snooping, Spurious Correlations, Biased Parameter Selection</p>
</blockquote>
<p>Developing machine learning models is no easy task. Feature engineering, hyperparameters optimization, data preparation and appropriate splitting for validation (to avoid snooping). These are just some of the challenges when taking on this endeavor.</p>
<p>Using tools like AutoML may help reduce the burden, however it won&rsquo;t take care of everything. In the end, you still need to understand your data characteristics and how it&rsquo;s related to your problem.</p>
<p>But, some of these relations and characteristics tend to repeat. Time relations, for example, are extremely common and important in a lot of Security problems. Therefore, I suggest that every Security team doing machine learning on its own to think about how they can extract such aspects into reusable components.</p>
<p>This has the added benefit to scale the impact of the Security members that are more focused on implementing machine learning. Our domain has many different areas and enabling other teams and specialists to more easily and correctly implement models, even as proofs of concept, expands the possibilities of what the enterprise can achieve.</p>
<p>In the end, the best strategy to best tackle all these challenges and be more prepared to handle pitfalls it to avoid jumping to complex and sexy algorithms from the start, such as neural networks.</p>
<p>Using explanation techniques, as the article suggests, can help you catch spurious correlation, but even better is to use simpler and more understandable methods. I have been seen great results with simple probabilistic methods, such as with Histogram Based Outlier Score. Using a simple algorithm, we can immediately see what is driving its decision and catch faster these types of pitfalls.</p>
<h3 id="performance-evaluation">Performance evaluation</h3>
<blockquote>
<p>Pitfalls: Inappropriate Baseline, Inappropriate Performance Measures, Base Rate Fallacy</p>
</blockquote>
<p>As I mentioned before, most times when building a learning-based system in Security, we end up with imbalanced and noisy datasets. This demands a special care when choosing performance metrics to best reflect our model. Building on the previous suggestion, having common components that use by default metrics more fit to most problems in Security, such as precision, recall, and MCC, would reduce the change of human error.</p>
<p>However, even after computing correct performance metrics, we still need good baselines to compare them against. The article suggest goods options such as using simple methods or automated machine learning. I would add that comparing it with vendor products can also be a good experiments to fully understand the impact of replacing the third party with the model.</p>
<h3 id="operation">Operation</h3>
<blockquote>
<p>Pitfalls: Lab-Only Evaluation, Inappropriate Threat Model</p>
</blockquote>
<p>Although the article identifies Lab-Only Evaluation as a common pitfall in Security model, I think this is one of the topic where in Security we have the most options to address.</p>
<p>First, we could use adversary emulation frameworks and popular attack toolkits to launch real world attacks against a test environment and study the model response. Another option would be to set up a honeypot and evaluate how well the model performed by comparing to a human analyses of the evidences afterward.</p>
<p>This type of experiment would also feed back into our threat model. Given the hype of AI in latest months, more knowledge is beings shared and produced about security best practices and process for machine learning, however we can still consider the topic in its infancy.</p>
<p>One special vector that we are still starting to discuss is supply-chain attacks to models, specially with the popularization of transfer-learning for more complex algorithms such as LLMs.</p>
<p><a href="https://www.splunk.com/en_us/blog/security/paws-in-the-pickle-jar-risk-vulnerability-in-the-model-sharing-ecosystem.html">Splunk showed</a> that more than 80% of HuggingFace&rsquo;s models use pickle-serialized code, which is vulnerable to arbitrary code execution and code injection, although it&rsquo;s not possible to say if any of them is malicious.</p>
<p>But, besides low-level vulnerabilities like this, a transfer-learning based model can also inherit the biases (intentionally placed or not) from the original model. That is, if the original model is vulnerable to an adversarial example, there is a high risk that your new model is going to also be.</p>
<p>This opens the possible for two types of attacks:</p>
<ol>
<li>Malicious models: base models shared with adversarial examples trained, such as a large batch of images with a negative label having a red square, making the model learn that any image with a red square should be classified as negative, therefore implanting a bypass. This type of attack would be extremely difficult to detect.</li>
<li>Cross model generalization: as <a href="https://arxiv.org/pdf/1312.6199.pdf">Szegedy, et all points in their paper</a>, different models may be susceptible to the same adversarial examples, even when having different hyperparameters. In this case, the models would not even be so different, therefore an adversary could search for adversarial examples against the original model and just use them in the target system, with a high chance of success.</li>
</ol>
<h2 id="conclusion">Conclusion</h2>
<p>In conclusion, implementing learning-based systems in the security domain presents challenges that require careful consideration. Addressing issues related to data collection, model design and implementation, performance evaluation, and operational considerations is crucial. By implementing these recommendations and reflecting on these problems, organizations can enhance their capabilities in security.</p>
]]></content:encoded>
    </item>
    
    <item>
      <title>Reflections about Supervised Learning on Security</title>
      <link>https://caioferreira.dev/posts/reflections-supervised-ml/reflections-supervised-learning-in-security/</link>
      <pubDate>Sun, 23 Apr 2023 00:00:00 +0000</pubDate>
      
      <guid>https://caioferreira.dev/posts/reflections-supervised-ml/reflections-supervised-learning-in-security/</guid>
      <description>Supervised learning is a technique that aims to learn a hypothesis function $h$ that fits a behavior observed in the real-world, which is governed by an unknown function $f$.
To learn this function, we use a set of example data points composed of inputs (also called features) and outcomes (sometimes called labels). These example data points were sampled from the real world behavior, i.e. from the function $f$, at some time in the past.</description>
      <content:encoded><![CDATA[<p>Supervised learning is a technique that aims to learn a hypothesis function $h$ that fits a behavior observed in the real-world, which is governed by an unknown function $f$.</p>
<p>To learn this function, we use a set of example data points composed of inputs (also called features) and outcomes (sometimes called labels). These example data points were sampled from the real world behavior, i.e. from the function $f$, at some time in the past. Our goal is that we can extract knowledge from the past to figure out this behavior on unseen data beforehand. This knowledge extracted from the set of examples is materialized on the $h$ function.</p>
<p>In Security, we can imagine some examples where this would be useful, like trying to learn if an HTTP request contains malicious payload or if some set of bytes is a malware or not. However, supervised learning is less often used in Security than in many other domains.</p>
<p>This happens because the principles of the supervised learning theory conflicts with the nature of Security, limiting its application. But, by understanding these principles, it is also possible to see how to best apply this technique and how it can maybe useful.</p>
<h2 id="stationary-assumption">Stationary assumption</h2>
<p>The most important principle in supervised learning is the stationary assumption. When the data that represents the real world behaviors follows the stationary assumption, it means that predicting the behavior using past example is approximately correctly.</p>
<p>The <strong>stationary assumption</strong> states that the behavior that is being learned don&rsquo;t changeÂ through time. This has some important consequences:</p>
<ol>
<li>We expect that each data point is independent of each other. This is important, because if there were causal effects between data points, then the features of a data point $x_1$, caused by $x_0$, would vary with a probability that is a combination of the probability distribution and the effect of $x_0$, hence the distribution would not remain the same over time, because $x_0$ and $x_1$ would vary in different ways. For example, in a box with 1 blue ball and 2 red balls, the probability of picking a blue ball when drawing the first one from the box is 33% while a red one would be 66%. However, if the first one is indeed blue, then the probability of drawing a red ball as the second one is 100%. The first data point (blue ball draw) changed the probability of the second data point.</li>
<li>We expect that each data point is identically distributed, i.e. each data point should be drawn from the same probability distribution. We could learn the shopping behavior using data ranging from Black Friday to New Years, however the users&rsquo; behavior in this time is completed different from the rest of the year, therefore the data used to learn has a different probability distribution than the unseen data on which we are going to make predictions.</li>
</ol>
<p>Any dataset that follows these two characteristics is said to hold the i.i.d assumption (independent and identically distributed). The importance for our training datasets on supervised learning to be i.i.d is because it connects the past to the future, without it, any inference made on the available data would be invalid.</p>
<p>Understand the i.i.d assumption is specially important for Security Machine Learning because it is one of the areas where causality and behavior shifts are most present. So, exactly how this assumption affects our ability to do Supervised Machine Learning?</p>
<h3 id="causality">Causality</h3>
<p>Many threat behaviors have causal nature, and therefore we should have a lot of care when preparing our datasets and choosing our validation methods.</p>
<p>A good example is malware classification, where you could have many samples from various families from different years. Each family generation influences each other, and sometimes they have similar characteristics.
A special bad situation that could happen with this is that during splitting of the dataset between training and testing, without taking into account the time relation of the families and samples, then you could end up training the model with future information and testing against past samples. This would produce falsely accurate results that would not generalize in the real-world.</p>
<p>There are ways to deal with this, but it depends on the type and strength of the causal relation between the data. For this case, ensuring that the newest malware samples are used for test should be enough.</p>
<h3 id="behavior-shift">Behavior Shift</h3>
<p>We could create a model to learn a threat behavior like the profile of a botnet, however once we started responding effectively to it, adversaries would adapt and our model would become useless because the new botnets would have a totally different behavior.
This would be the case of a change in the probability distribution from which the features are drawn, leading to the break of our assumption.</p>
<h2 id="looking-to-the-other-side">Looking to the other side</h2>
<p>Although causality may be addressed by good data preparation, preventing a model to be become outdated due to behavioral shift is almost impossible. However, this problem isn&rsquo;t a new one in Security, such that one best practices is to instead of trying to detect and block malicious action, defenders should define what a legitimate system behavior looks like and block everything else. This can be summarized as: allow lists are more secure than block lists.</p>
<p>We can apply the same philosophy for supervised learning, by modeling profiles of legitimate behavior, which usually are more stable. Then, new data points are classified against these multiple profiles models, finally a meta-classifier is used to choose which one is the best fit or if it&rsquo;s an outlier. This has the ability to catch any new threat behavior that deviates from the know legitimate profiles.</p>
<p>This combination of multiple models is called ensemble learning, which has shown to improve models performance, like when comparing a Decision Tree model to a Random Forest one.</p>
<p>As a side bonus, since this way of building models does not depend on knowing threat behaviors, it avoids the common problem in mining data for Security that usually produce highly unbalanced datasets. We can train each profile using the true positive data of others profiles as false examples, assuming each profile is mutually exclusive.</p>
<p>The main challenge in this approach is that adversaries may try to mimic legitimate profiles, however like the authors of Notos showed, this can be useful sometimes, as in their cases this would imply in adversaries using a more stable network infrastructure that would be easily defeated by static block lists. Therefore, forcing adversaries to mimic a legitimate profile could also reduce their capabilities.</p>
<h2 id="conclusion">Conclusion</h2>
<p>In conclusion, supervised learning is a powerful technique that allows us to extract knowledge from past data points to predict future behavior. However, in the Security domain, applying it comes with unique challenges due to the presence of causality and behavior shifts. Understanding the stationary assumption and the importance of having an i.i.d dataset is crucial, because it informs us how to best prepare our datasets, choose our validation methods and, most importantly, what are the best behaviors to be modeled using supervised learning.</p>
<p>Luckily, by modeling profiles of legitimate behavior, and catching new threat behavior that deviates from known legitimate profiles, we can build intelligent allow lists.</p>
<p>While there are challenges, with proper preparation and understanding, supervised learning can be a valuable tool in Security.</p>
<h2 id="references">References</h2>
<ul>
<li><a href="https://astrolavos.gatech.edu/articles/Antonakakis.pdf">Notos: Building a Dynamic Reputation System for DNS</a></li>
</ul>
]]></content:encoded>
    </item>
    
  </channel>
</rss>
