<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Notes on Dos and Dont's of Machine Learning in Computer Security | Simple Made Daily</title><meta name=keywords content="ml,security,supervised-learning,ai"><meta name=description content="Following the subject from my last post, Reflections about Supervised Learning on Security, I put down some more thoughts about the implementation of learning-based systems in the Security domain.
This is my extension to the problems and recommendations presented on the paper Dos and Don&rsquo;ts of Machine Learning in Computer Security (Quiring, et al, 2022). I encourage you to also read the paper, as it&rsquo;s excellent and provide a lot of insights about how to better build machine learning models."><meta name=author content="Caio Ferreira"><link rel=canonical href=https://caioferreira.dev/posts/notes-on-do-and-donts-of-ml-in-security/notes-on-do-and-donts-of-ml-in-security/><meta name=google-site-verification content="G-520NXKH2M6"><link crossorigin=anonymous href=/assets/css/stylesheet.d53593ac5b971ecf76b17a47ca5e9838cbe9f27c71a39c42e20c4361a252c9c0.css integrity="sha256-1TWTrFuXHs92sXpHyl6YOMvp8nxxo5xC4gxDYaJSycA=" rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG+9vmJ0cTS+ovo0FeA=" onload=hljs.initHighlightingOnLoad()></script>
<link rel=icon href=https://caioferreira.dev/favicon.png><link rel=icon type=image/png sizes=16x16 href=https://caioferreira.dev/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://caioferreira.dev/favicon-32x32.png><link rel=apple-touch-icon href=https://caioferreira.dev/apple-touch-icon.png><link rel=mask-icon href=https://caioferreira.dev/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.1/styles/atom-one-dark.min.css integrity="sha512-Fcqyubi5qOvl+yCwSJ+r7lli+CO1eHXMaugsZrnxuU4DVpLYWXTVoHy55+mCb4VZpMgy7PBhV7IiymC0yu9tkQ==" crossorigin=anonymous referrerpolicy=no-referrer><script async src="https://www.googletagmanager.com/gtag/js?id=G-520NXKH2M6"></script>
<script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-520NXKH2M6",{anonymize_ip:!1})}</script><meta property="og:title" content="Notes on Dos and Dont's of Machine Learning in Computer Security"><meta property="og:description" content="Following the subject from my last post, Reflections about Supervised Learning on Security, I put down some more thoughts about the implementation of learning-based systems in the Security domain.
This is my extension to the problems and recommendations presented on the paper Dos and Don&rsquo;ts of Machine Learning in Computer Security (Quiring, et al, 2022). I encourage you to also read the paper, as it&rsquo;s excellent and provide a lot of insights about how to better build machine learning models."><meta property="og:type" content="article"><meta property="og:url" content="https://caioferreira.dev/posts/notes-on-do-and-donts-of-ml-in-security/notes-on-do-and-donts-of-ml-in-security/"><meta property="og:image" content="https://caioferreira.dev/blog-cover.png"><meta property="article:section" content="posts"><meta property="article:published_time" content="2023-06-22T00:00:00+00:00"><meta property="article:modified_time" content="2023-06-22T00:00:00+00:00"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://caioferreira.dev/blog-cover.png"><meta name=twitter:title content="Notes on Dos and Dont's of Machine Learning in Computer Security"><meta name=twitter:description content="Following the subject from my last post, Reflections about Supervised Learning on Security, I put down some more thoughts about the implementation of learning-based systems in the Security domain.
This is my extension to the problems and recommendations presented on the paper Dos and Don&rsquo;ts of Machine Learning in Computer Security (Quiring, et al, 2022). I encourage you to also read the paper, as it&rsquo;s excellent and provide a lot of insights about how to better build machine learning models."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://caioferreira.dev/posts/"},{"@type":"ListItem","position":2,"name":"Notes on Dos and Dont's of Machine Learning in Computer Security","item":"https://caioferreira.dev/posts/notes-on-do-and-donts-of-ml-in-security/notes-on-do-and-donts-of-ml-in-security/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Notes on Dos and Dont's of Machine Learning in Computer Security","name":"Notes on Dos and Dont\u0027s of Machine Learning in Computer Security","description":"Following the subject from my last post, Reflections about Supervised Learning on Security, I put down some more thoughts about the implementation of learning-based systems in the Security domain.\nThis is my extension to the problems and recommendations presented on the paper Dos and Don\u0026rsquo;ts of Machine Learning in Computer Security (Quiring, et al, 2022). I encourage you to also read the paper, as it\u0026rsquo;s excellent and provide a lot of insights about how to better build machine learning models.","keywords":["ml","security","supervised-learning","ai"],"articleBody":"Following the subject from my last post, Reflections about Supervised Learning on Security, I put down some more thoughts about the implementation of learning-based systems in the Security domain.\nThis is my extension to the problems and recommendations presented on the paper Dos and Don’ts of Machine Learning in Computer Security (Quiring, et al, 2022). I encourage you to also read the paper, as it’s excellent and provide a lot of insights about how to better build machine learning models.\nMachine learning workflow Data collection Pitfalls: Sampling Bias, Label Inaccuracy\nAs with any other domain, Security is also heavily affected by bad data quality used for training. However, it’s often worse in Security because data acquisition of adversary activity is usually hard and the method’s used to acquire it will bring a bias.\nLet’s take for example a honeypot implemented with a vulnerable Apache Server. Even though there are lots of bad actors in the wild, if you have a medium size environment, the volume of data they will produce attacking your honeypot will not come near to the volume of packets in your production network. Also, the TTPs you are going to see will come from threat actors that are used to leverage Apache Server in their kill chain, possibly leaving other adversaries, that may be focusing on other types of vectors, producing different kill chains, out of the dataset.\nOn top of that, we are usually dealing with lots of examples. So, if we have label inaccuracies, dealing with them is a lot harder. We can’t just apply common techniques like using the mode to fill in, because in the Security domain, the adversary is actively trying to mimic the distribution of the benign cases. Therefore, we have very little space for noise, as it would blur even more the distinction between the outcomes.\nBesides the recommendations presented in the article, I would add two more:\nUser open and disseminate datasets whenever possible. Dataset sharing is still uncommon on the Security community. We already have some network and host datasets, but there are many more assets nowadays (logs for cloud, kubernetes, CDN, CI/CD, etc), and the technologies on networks and hosts are constantly evolving, posing the need for these datasets to be always updated. Use a model design that depends less on adversary data, such as we discussed in the previous article. This will reduce the dependency on the adversary behavior and increase the sources of data available to use. Model design and implementation Pitfalls: Data Snooping, Spurious Correlations, Biased Parameter Selection\nDeveloping machine learning models is no easy task. Feature engineering, hyperparameters optimization, data preparation and appropriate splitting for validation (to avoid snooping). These are just some of the challenges when taking on this endeavor.\nUsing tools like AutoML may help reduce the burden, however it won’t take care of everything. In the end, you still need to understand your data characteristics and how it’s related to your problem.\nBut, some of these relations and characteristics tend to repeat. Time relations, for example, are extremely common and important in a lot of Security problems. Therefore, I suggest that every Security team doing machine learning on its own to think about how they can extract such aspects into reusable components.\nThis has the added benefit to scale the impact of the Security members that are more focused on implementing machine learning. Our domain has many different areas and enabling other teams and specialists to more easily and correctly implement models, even as proofs of concept, expands the possibilities of what the enterprise can achieve.\nIn the end, the best strategy to best tackle all these challenges and be more prepared to handle pitfalls it to avoid jumping to complex and sexy algorithms from the start, such as neural networks.\nUsing explanation techniques, as the article suggests, can help you catch spurious correlation, but even better is to use simpler and more understandable methods. I have been seen great results with simple probabilistic methods, such as with Histogram Based Outlier Score. Using a simple algorithm, we can immediately see what is driving its decision and catch faster these types of pitfalls.\nPerformance evaluation Pitfalls: Inappropriate Baseline, Inappropriate Performance Measures, Base Rate Fallacy\nAs I mentioned before, most times when building a learning-based system in Security, we end up with imbalanced and noisy datasets. This demands a special care when choosing performance metrics to best reflect our model. Building on the previous suggestion, having common components that use by default metrics more fit to most problems in Security, such as precision, recall, and MCC, would reduce the change of human error.\nHowever, even after computing correct performance metrics, we still need good baselines to compare them against. The article suggest goods options such as using simple methods or automated machine learning. I would add that comparing it with vendor products can also be a good experiments to fully understand the impact of replacing the third party with the model.\nOperation Pitfalls: Lab-Only Evaluation, Inappropriate Threat Model\nAlthough the article identifies Lab-Only Evaluation as a common pitfall in Security model, I think this is one of the topic where in Security we have the most options to address.\nFirst, we could use adversary emulation frameworks and popular attack toolkits to launch real world attacks against a test environment and study the model response. Another option would be to set up a honeypot and evaluate how well the model performed by comparing to a human analyses of the evidences afterward.\nThis type of experiment would also feed back into our threat model. Given the hype of AI in latest months, more knowledge is beings shared and produced about security best practices and process for machine learning, however we can still consider the topic in its infancy.\nOne special vector that we are still starting to discuss is supply-chain attacks to models, specially with the popularization of transfer-learning for more complex algorithms such as LLMs.\nSplunk showed that more than 80% of HuggingFace’s models use pickle-serialized code, which is vulnerable to arbitrary code execution and code injection, although it’s not possible to say if any of them is malicious.\nBut, besides low-level vulnerabilities like this, a transfer-learning based model can also inherit the biases (intentionally placed or not) from the original model. That is, if the original model is vulnerable to an adversarial example, there is a high risk that your new model is going to also be.\nThis opens the possible for two types of attacks:\nMalicious models: base models shared with adversarial examples trained, such as a large batch of images with a negative label having a red square, making the model learn that any image with a red square should be classified as negative, therefore implanting a bypass. This type of attack would be extremely difficult to detect. Cross model generalization: as Szegedy, et all points in their paper, different models may be susceptible to the same adversarial examples, even when having different hyperparameters. In this case, the models would not even be so different, therefore an adversary could search for adversarial examples against the original model and just use them in the target system, with a high chance of success. Conclusion In conclusion, implementing learning-based systems in the security domain presents challenges that require careful consideration. Addressing issues related to data collection, model design and implementation, performance evaluation, and operational considerations is crucial. By implementing these recommendations and reflecting on these problems, organizations can enhance their capabilities in security.\n","wordCount":"1250","inLanguage":"en","datePublished":"2023-06-22T00:00:00Z","dateModified":"2023-06-22T00:00:00Z","author":{"@type":"Person","name":"Caio Ferreira"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://caioferreira.dev/posts/notes-on-do-and-donts-of-ml-in-security/notes-on-do-and-donts-of-ml-in-security/"},"publisher":{"@type":"Organization","name":"Simple Made Daily","logo":{"@type":"ImageObject","url":"https://caioferreira.dev/favicon.png"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://caioferreira.dev/ accesskey=h title="Simple Made Daily (Alt + H)"><img src=https://caioferreira.dev/logo.png alt aria-label=logo height=28>Simple Made Daily</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://caioferreira.dev/posts/ title=Blog><span>Blog</span></a></li><li><a href=https://caioferreira.dev/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li><li><a href=https://caioferreira.dev/tags/ title=Tags><span>Tags</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class=post-title>Notes on Dos and Dont's of Machine Learning in Computer Security</h1><div class=post-meta><span title='2023-06-22 00:00:00 +0000 UTC'>June 22, 2023</span>&nbsp;·&nbsp;6 min&nbsp;·&nbsp;Caio Ferreira&nbsp;|&nbsp;<a href=https://github.com/caiorcferreira/caioferreira.dev/tree/main/content/posts/notes-on-do-and-donts-of-ml-in-security/notes-on-do-and-donts-of-ml-in-security.md rel="noopener noreferrer" target=_blank>Suggest Changes</a></div></header><div class=post-content><p>Following the subject from my last post, <a href=https://caioferreira.dev/posts/reflections-supervised-ml/reflections-supervised-learning-in-security/>Reflections about Supervised Learning on Security</a>, I put down some more thoughts about the implementation of learning-based systems in the Security domain.</p><p>This is my extension to the problems and recommendations presented on the paper <a href=https://mlsec.org/docs/2022-sec.pdf>Dos and Don&rsquo;ts of Machine Learning in Computer Security</a> (Quiring, et al, 2022). I encourage you to also read the paper, as it&rsquo;s excellent and provide a lot of insights about how to better build machine learning models.</p><h2 id=machine-learning-workflow>Machine learning workflow<a hidden class=anchor aria-hidden=true href=#machine-learning-workflow>#</a></h2><h3 id=data-collection>Data collection<a hidden class=anchor aria-hidden=true href=#data-collection>#</a></h3><blockquote><p>Pitfalls: Sampling Bias, Label Inaccuracy</p></blockquote><p>As with any other domain, Security is also heavily affected by bad data quality used for training. However, it&rsquo;s often worse in Security because data acquisition of adversary activity is usually hard and the method&rsquo;s used to acquire it will bring a bias.</p><p>Let&rsquo;s take for example a honeypot implemented with a vulnerable Apache Server. Even though there are lots of bad actors in the wild, if you have a medium size environment, the volume of data they will produce attacking your honeypot will not come near to the volume of packets in your production network. Also, the TTPs you are going to see will come from threat actors that are used to leverage Apache Server in their kill chain, possibly leaving other adversaries, that may be focusing on other types of vectors, producing different kill chains, out of the dataset.</p><p>On top of that, we are usually dealing with lots of examples. So, if we have label inaccuracies, dealing with them is a lot harder. We can&rsquo;t just apply common techniques like using the mode to fill in, because in the Security domain, the adversary is actively trying to mimic the distribution of the benign cases. Therefore, we have very little space for noise, as it would blur even more the distinction between the outcomes.</p><p>Besides the recommendations presented in the article, I would add two more:</p><ul><li>User open and disseminate datasets whenever possible. Dataset sharing is still uncommon on the Security community. We already have some network and host datasets, but there are many more assets nowadays (logs for cloud, kubernetes, CDN, CI/CD, etc), and the technologies on networks and hosts are constantly evolving, posing the need for these datasets to be always updated.</li><li>Use a model design that depends less on adversary data, such as we discussed in the previous article. This will reduce the dependency on the adversary behavior and increase the sources of data available to use.</li></ul><h3 id=model-design-and-implementation>Model design and implementation<a hidden class=anchor aria-hidden=true href=#model-design-and-implementation>#</a></h3><blockquote><p>Pitfalls: Data Snooping, Spurious Correlations, Biased Parameter Selection</p></blockquote><p>Developing machine learning models is no easy task. Feature engineering, hyperparameters optimization, data preparation and appropriate splitting for validation (to avoid snooping). These are just some of the challenges when taking on this endeavor.</p><p>Using tools like AutoML may help reduce the burden, however it won&rsquo;t take care of everything. In the end, you still need to understand your data characteristics and how it&rsquo;s related to your problem.</p><p>But, some of these relations and characteristics tend to repeat. Time relations, for example, are extremely common and important in a lot of Security problems. Therefore, I suggest that every Security team doing machine learning on its own to think about how they can extract such aspects into reusable components.</p><p>This has the added benefit to scale the impact of the Security members that are more focused on implementing machine learning. Our domain has many different areas and enabling other teams and specialists to more easily and correctly implement models, even as proofs of concept, expands the possibilities of what the enterprise can achieve.</p><p>In the end, the best strategy to best tackle all these challenges and be more prepared to handle pitfalls it to avoid jumping to complex and sexy algorithms from the start, such as neural networks.</p><p>Using explanation techniques, as the article suggests, can help you catch spurious correlation, but even better is to use simpler and more understandable methods. I have been seen great results with simple probabilistic methods, such as with Histogram Based Outlier Score. Using a simple algorithm, we can immediately see what is driving its decision and catch faster these types of pitfalls.</p><h3 id=performance-evaluation>Performance evaluation<a hidden class=anchor aria-hidden=true href=#performance-evaluation>#</a></h3><blockquote><p>Pitfalls: Inappropriate Baseline, Inappropriate Performance Measures, Base Rate Fallacy</p></blockquote><p>As I mentioned before, most times when building a learning-based system in Security, we end up with imbalanced and noisy datasets. This demands a special care when choosing performance metrics to best reflect our model. Building on the previous suggestion, having common components that use by default metrics more fit to most problems in Security, such as precision, recall, and MCC, would reduce the change of human error.</p><p>However, even after computing correct performance metrics, we still need good baselines to compare them against. The article suggest goods options such as using simple methods or automated machine learning. I would add that comparing it with vendor products can also be a good experiments to fully understand the impact of replacing the third party with the model.</p><h3 id=operation>Operation<a hidden class=anchor aria-hidden=true href=#operation>#</a></h3><blockquote><p>Pitfalls: Lab-Only Evaluation, Inappropriate Threat Model</p></blockquote><p>Although the article identifies Lab-Only Evaluation as a common pitfall in Security model, I think this is one of the topic where in Security we have the most options to address.</p><p>First, we could use adversary emulation frameworks and popular attack toolkits to launch real world attacks against a test environment and study the model response. Another option would be to set up a honeypot and evaluate how well the model performed by comparing to a human analyses of the evidences afterward.</p><p>This type of experiment would also feed back into our threat model. Given the hype of AI in latest months, more knowledge is beings shared and produced about security best practices and process for machine learning, however we can still consider the topic in its infancy.</p><p>One special vector that we are still starting to discuss is supply-chain attacks to models, specially with the popularization of transfer-learning for more complex algorithms such as LLMs.</p><p><a href=https://www.splunk.com/en_us/blog/security/paws-in-the-pickle-jar-risk-vulnerability-in-the-model-sharing-ecosystem.html>Splunk showed</a> that more than 80% of HuggingFace&rsquo;s models use pickle-serialized code, which is vulnerable to arbitrary code execution and code injection, although it&rsquo;s not possible to say if any of them is malicious.</p><p>But, besides low-level vulnerabilities like this, a transfer-learning based model can also inherit the biases (intentionally placed or not) from the original model. That is, if the original model is vulnerable to an adversarial example, there is a high risk that your new model is going to also be.</p><p>This opens the possible for two types of attacks:</p><ol><li>Malicious models: base models shared with adversarial examples trained, such as a large batch of images with a negative label having a red square, making the model learn that any image with a red square should be classified as negative, therefore implanting a bypass. This type of attack would be extremely difficult to detect.</li><li>Cross model generalization: as <a href=https://arxiv.org/pdf/1312.6199.pdf>Szegedy, et all points in their paper</a>, different models may be susceptible to the same adversarial examples, even when having different hyperparameters. In this case, the models would not even be so different, therefore an adversary could search for adversarial examples against the original model and just use them in the target system, with a high chance of success.</li></ol><h2 id=conclusion>Conclusion<a hidden class=anchor aria-hidden=true href=#conclusion>#</a></h2><p>In conclusion, implementing learning-based systems in the security domain presents challenges that require careful consideration. Addressing issues related to data collection, model design and implementation, performance evaluation, and operational considerations is crucial. By implementing these recommendations and reflecting on these problems, organizations can enhance their capabilities in security.</p></div><footer class=post-footer><ul class=post-tags><li><a href=https://caioferreira.dev/tags/ml/>ml</a></li><li><a href=https://caioferreira.dev/tags/security/>security</a></li><li><a href=https://caioferreira.dev/tags/supervised-learning/>supervised-learning</a></li><li><a href=https://caioferreira.dev/tags/ai/>ai</a></li></ul><div class=share-buttons><a target=_blank rel="noopener noreferrer" aria-label="share Notes on Dos and Dont's of Machine Learning in Computer Security on twitter" href="https://twitter.com/intent/tweet/?text=Notes%20on%20Dos%20and%20Dont%27s%20of%20Machine%20Learning%20in%20Computer%20Security&amp;url=https%3a%2f%2fcaioferreira.dev%2fposts%2fnotes-on-do-and-donts-of-ml-in-security%2fnotes-on-do-and-donts-of-ml-in-security%2f&amp;hashtags=ml%2csecurity%2csupervised-learning%2cai"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM195.519 424.544c135.939.0 210.268-112.643 210.268-210.268.0-3.218.0-6.437-.153-9.502 14.406-10.421 26.973-23.448 36.935-38.314-13.18 5.824-27.433 9.809-42.452 11.648 15.326-9.196 26.973-23.602 32.49-40.92-14.252 8.429-30.038 14.56-46.896 17.931-13.487-14.406-32.644-23.295-53.946-23.295-40.767.0-73.87 33.104-73.87 73.87.0 5.824.613 11.494 1.992 16.858-61.456-3.065-115.862-32.49-152.337-77.241-6.284 10.881-9.962 23.601-9.962 37.088.0 25.594 13.027 48.276 32.95 61.456-12.107-.307-23.448-3.678-33.41-9.196v.92c0 35.862 25.441 65.594 59.311 72.49-6.13 1.686-12.72 2.606-19.464 2.606-4.751.0-9.348-.46-13.946-1.38 9.349 29.426 36.628 50.728 68.965 51.341-25.287 19.771-57.164 31.571-91.8 31.571-5.977.0-11.801-.306-17.625-1.073 32.337 21.15 71.264 33.41 112.95 33.41z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Notes on Dos and Dont's of Machine Learning in Computer Security on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fcaioferreira.dev%2fposts%2fnotes-on-do-and-donts-of-ml-in-security%2fnotes-on-do-and-donts-of-ml-in-security%2f&amp;title=Notes%20on%20Dos%20and%20Dont%27s%20of%20Machine%20Learning%20in%20Computer%20Security&amp;summary=Notes%20on%20Dos%20and%20Dont%27s%20of%20Machine%20Learning%20in%20Computer%20Security&amp;source=https%3a%2f%2fcaioferreira.dev%2fposts%2fnotes-on-do-and-donts-of-ml-in-security%2fnotes-on-do-and-donts-of-ml-in-security%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Notes on Dos and Dont's of Machine Learning in Computer Security on reddit" href="https://reddit.com/submit?url=https%3a%2f%2fcaioferreira.dev%2fposts%2fnotes-on-do-and-donts-of-ml-in-security%2fnotes-on-do-and-donts-of-ml-in-security%2f&title=Notes%20on%20Dos%20and%20Dont%27s%20of%20Machine%20Learning%20in%20Computer%20Security"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a></div></footer></article></main><footer class=footer><span>&copy; 2023 <a href=https://caioferreira.dev/>Simple Made Daily</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>